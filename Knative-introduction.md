> This is an excerpt from the excellent [Red Hat Developer Knative Tutorial](https://redhat-developer-demos.github.io/knative-tutorial/).
> Check the original content if you got more than 1h30 in front of you :wink:

## Pre-requisite

* Get a Red Hat Developer account at https://developers.redhat.com
* Initialize your Developer Sandbox at https://developers.redhat.com/developer-sandbox/get-started

## Preparation

In your Developer Sandbox, open the **Web Terminal** tool on the right of header bar. Once connected in the terminal, initialized the current Kubernetes context on your `<username>-stage` namespace.

```sh
kubectl config set-context --current --namespace=laurent-broudoux-stage
```

Ensure you have the minimal `kn` CLI tool version available:

```sh
$ kn version
# --- OUTPUT ---
Version:      v1.1.0
Build Date:   2021-12-14T12:31:50Z
Git Revision: 530841f1
Supported APIs:
* Serving
  - serving.knative.dev/v1 (knative-serving )
* Eventing
  - sources.knative.dev/v1 (knative-eventing )
  - eventing.knative.dev/v1 (knative-eventing )
```

## Serving basics

Knative Serving is ideal for running your application services inside Kubernetes by providing a more simplified deployment syntax with automated scale-to-zero and scale-out based on HTTP load. The Knative platform will manage your serviceâ€™s deployments, revisions, networking and scaling. Knative Serving exposes your service via an HTTP URL and has a lot of sane defaults for its configurations. For many practical use cases you might need to tweak the defaults to your needs and might also need to adjust the traffic distribution amongst the service revisions.

Deploy a new KService:

```sh
kn service create greeter \
  --image=quay.io/rhdevelopers/knative-tutorial-greeter:quarkus
```

Inspect what's going on in the OpenShift DevSandbox developer console and check service is present through the CLI:

```sh
$ kn service list
NAME      URL                                                                             LATEST          AGE   CONDITIONS   READY   REASON
greeter   https://greeter-laurent-broudoux-stage.apps.sandbox.x8i5.p1.openshiftapps.com   greeter-00001   74s   3 OK / 3 
```

Inspect service through the CLI and retrieve URL to test it:

```sh
kn service describe greeter
kn service describe greeter -o url
curl $(kn service describe greeter -o url)
```

## Service updating / revisions

To create a new revision using `kn` is as easy as running another command.

```sh
kn service update greeter --env "MESSAGE_PREFIX=Namaste"
```

Now invoking the service will return a response like:

```sh
$ curl $(kn service describe greeter -o url)
# --- OUTPUT ---
Namaste  greeter => '9861675f8845' : 1
```

List all the revisions of this service with:

```sh
kn service describe greeter -v
```

You can also check the list of revisions and describe a specific one:

```sh
kn revision list
kn revision describe greeter-00001
kn revision describe greeter-00002
```

You can also check the Knative routes available:

```sh
kn route list
```

## Serving configuration

The Knative configuration is the top-level resources for revisions:

```sh
$ kubectl get configurations.serving.knative.dev greeter
# --- OUTPUT ----
NAME      LATESTCREATED   LATESTREADY     READY   REASON
greeter   greeter-00002   greeter-00002   True 
```

## Serving revisions and traffic distribution

As you learnt from previous chapter, Knative always routes traffic to the **latest** revision of the service. It is possible to split the traffic amongst the available revisions.

To explore the goals of this chapter, we will be using a colors service called **blue-green-canary**.

```sh
kn service create blue-green-canary \
   --image=quay.io/rhdevelopers/blue-green-canary \
   --env BLUE_GREEN_CANARY_COLOR="#6bbded" \
   --env BLUE_GREEN_CANARY_MESSAGE="Hello"
```

Get the service URL and open it into a browser window:

```sh
kn service describe blue-green-canary -o url
```

Let us now change the configuration of the service by updating the service environment variable `BLUE_GREEN_CANARY_COLOR` to make the browser display `green` color with greeting text as `Namaste`.

```sh
kn service update blue-green-canary \
   --image=quay.io/rhdevelopers/blue-green-canary \
   --env BLUE_GREEN_CANARY_COLOR="#5bbf45" \
   --env BLUE_GREEN_CANARY_MESSAGE="Namaste"
```

Let's check revisions with following commands:

```sh
kn revision list
kn revision list -s blue-green-canary
```

### Tagging

As you had observed that the Knative service `blue-green-canary` now has two revisions namely `blue-green-canary-00001` and `blue-green-canary-00002`. As the Revision names are autogenerated it is hard to comprehend to which code/configuration set it corresponds to. To overcome this problem Knative provides **tagging of revision names** that allows one to tag a revision name to a logical human understandable names called tags.

As our colors service shows different colors on the browser let us tag the revisions with colors:

```sh
kn service update blue-green-canary --tag=blue-green-canary-00001=blue
kn service update blue-green-canary --tag=blue-green-canary-00002=green
kn revision list -s blue-green-canary
```

Lets tag whatever revision that is latest to be tagged as **latest**.

```sh
kn service update blue-green-canary --tag=@latest=latest
kn revision list -s blue-green-canary
```

As green happened to be latest revision it has been tagged with name latest in addition to green.

Let us use the tag names for easier identification of the revision and perform traffic distribution amongst them.

### Splitting traffic

#### Blue-Green deployment

Knative offers a simple way of switching 100% of the traffic from one Knative service revision (blue) to another newly rolled out revision (green). If the new revision (e.g. green) has erroneous behavior then it is easy to rollback the change.

```sh
kn service update blue-green-canary --traffic blue=100,green=0,latest=0
kn revision list -s blue-green-canary
```

Other routes are still there, list them with following commands:

```sh
kubectl get ksvc blue-green-canary -o yaml | yq r - 'status.traffic[*].url'
kubectl get ksvc blue-green-canary -o json | jq .status.traffic[].url
```

#### Canary release

A Canary release is more effective when you want to reduce the risk of introducing new feature. It allows you a more effective feature-feedback loop before rolling out the change to your entire user base.

Knative allows you to split the traffic between revisions in increments as small as 1%.

```sh
kn service update blue-green-canary \
  --traffic="blue=80" \
  --traffic="green=20"
kn revision list -s blue-green-canary
```

You can the change splitting weight and check explicit revision URLs:

```sh
kubectl get ksvc blue-green-canary -o yaml | yq r - 'status.traffic[*].url'
```

## Scaling

> On OpenShift Dev Sandbox, be careful to free resources before scaling...

By default Knative Serving allows 100 concurrent requests into a pod. This is defined by the `container-concurrency-target-default` setting in the configmap `config-autoscaler` in the `knative-serving` namespace.

For this exercise let us make our service handle only 10 concurrent requests. This will cause Knative autoscaler to scale to more pods as soon as we run more than 10 requests in parallel against the revision.

Deploy a new Knative service:

```sh
kn service create prime-generator \
  --concurrency-target=10 \
  --image=quay.io/rhdevelopers/prime-generator:v27-quarkus
```

Get the public url of this Service and load it using `hey`:

```sh
$ export SVC_URL=$(kn service describe prime-generator -o url)
$ hey -c 50 -z 10s "$SVC_URL/?sleep=3&upto=10000&memload=100"
```

You will notice the number of greeter service pods will have scaled to 5 or more pods automatically.

f your app needs to stay particularly responsive under any circumstances and/or has a long startup time, it might be beneficial to always keep a minimum number of pods around. This can be done via an the annotation autoscaling.`knative.dev/minScale`.

Let's update our service:

```sh
kn service update prime-generator --scale-min=2
```

## Event basics

**Knative Eventing Sources** are software components that emit events. The job of a Source is to connect to, drain, capture and potentially buffer events; often from an external system and then relay those events to the **Sink**.

> [CloudEvents](https://cloudevents.io/) is a specification for describing event data in a common way. An event might be produced by any number of sources (e.g. Kafka, S3, GCP PubSub, MQTT) and as a software developer, you want a common abstraction for all event inputs.

Deploy `eventinghello` service and create a source (also possible OpenShift developer console, but take care of `data` field in this case)

```sh
kn service create eventinghello \
  --concurrency-target=5 \
  --image=quay.io/rhdevelopers/eventinghello:0.0.2

kn source ping create eventinghello-ping-source \
  --schedule "*/2 * * * *" \
  --data '{"message": "Thanks for doing Knative Tutorial"}' \
  --sink ksvc:eventinghello
```

Explore the content of a Source:

```sh
kn source list
kn source ping list
kn source ping describe ping-source
```

## Channels and Subscriptions

**Channels**

Channels are an event forwarding and persistence layer where each channel is a separate Kubernetes Custom Resource. A Channel may be backed by Apache Kafka or InMemoryChannel. This recipe focuses on InMemoryChannel.

**Subscriptions**

Subscriptions are how you register your service to listen to a particular channel.

> On OpenShift Dev Sandbox, you may need to free some resources like the Knative `greeter` we previoudly deploy: `kn service delete greeter`

```sh
kn channel create eventinghello-ch

kn channel list
kn channel describe eventinghello-ch

kn source ping create event-greeter-ping-source \
  --schedule "*/2 * * * *" \
  --data '{"message": "Thanks for subscribing Channel"}' \
  --sink channel:eventinghello-ch

kn subscription create eventinghello-sub \
  --channel eventinghello-ch \
  --sink ksvc:eventinghello

kn service create eventinghelloa \
  --concurrency-target=5 \
  --image=quay.io/rhdevelopers/eventinghello:0.0.2

kn subscription create eventinghelloa-sub \
  --channel eventinghello-ch \
  --sink ksvc:eventinghelloa
```
